{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-Hnx_4qesFq",
        "outputId": "50f61aee-1df4-4b8f-f628-6c4f676fa36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FLSAhUNhCAa",
        "outputId": "be143ebf-8e0b-4873-a313-1546cca7072b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
            "0  0.020622 -0.043543  0.017362  0.024762 -0.020766 -0.017783 -0.023139   \n",
            "1 -0.014496 -0.008514  0.001838 -0.013815 -0.023862  0.021470 -0.007265   \n",
            "2  0.021526 -0.021832  0.063153 -0.033886 -0.009450 -0.035176  0.080432   \n",
            "3 -0.047248  0.018531 -0.021601  0.054370 -0.024639  0.022130  0.068030   \n",
            "4  0.038407 -0.026316 -0.016674 -0.013441  0.014775  0.005128  0.034026   \n",
            "\n",
            "      dim_8     dim_9    dim_10  ...   dim_298   dim_299   dim_300  chunk  \\\n",
            "0  0.015411 -0.021622 -0.045250  ... -0.002109 -0.018631 -0.002061     NP   \n",
            "1  0.014412  0.024713  0.020350  ... -0.026965  0.007601  0.007057    NP2   \n",
            "2  0.158831 -0.014669 -0.037460  ...  0.101427 -0.090182  0.008483   VGNF   \n",
            "3 -0.017688  0.005726 -0.042218  ...  0.032187  0.065817  0.067730    NP3   \n",
            "4  0.008274 -0.023595 -0.039830  ...  0.047305 -0.050628 -0.014780    NP4   \n",
            "\n",
            "   postposition  head-postag   dependency  is_arg       srl  predicate  \n",
            "0            का          NP2           r6     0.0       NaN        NaN  \n",
            "1           NaN         VGNF           k2     1.0      ARG1       VGNF  \n",
            "2         हो+एं          NP4  nmod__k1inv     0.0       NaN        NaN  \n",
            "3           में         VGNF          k7p     1.0  ARGM-LOC       VGNF  \n",
            "4            का          VGF           k1     1.0      ARG0        VGF  \n",
            "\n",
            "[5 rows x 307 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path =  \"/content/drive/MyDrive/final_data.csv\"\n",
        "# file_path = \"final_data.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEO4wGyZF-b_"
      },
      "source": [
        "**DATASET**\n",
        "- For the dataset we have retrieved the dataset from a paper source, the dataset contains the word embeddings of 300 dimension\n",
        "- Along with this the dataset also contain the columms with posposition, head-POS , srl , predicate and etc\n",
        "- But out of these we just need the embeddings for the classification and the label would the be SRL\n",
        "- So for the preparation of the data, we just use these attributes as for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrzkic_6jRiP",
        "outputId": "f856c0b8-f2e9-4b28-eb28-b54de4c67cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0             NaN\n",
            "1            ARG1\n",
            "2             NaN\n",
            "3        ARGM-LOC\n",
            "4            ARG0\n",
            "           ...   \n",
            "14407         NaN\n",
            "14408    ARGM-PRP\n",
            "14409    ARGM-LOC\n",
            "14410        ARG1\n",
            "14411         NaN\n",
            "Name: srl, Length: 14412, dtype: object\n"
          ]
        }
      ],
      "source": [
        "labels = data['srl']\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83JkXM_sj5a0",
        "outputId": "768fe48f-d31d-4533-f72d-0f1b84359e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ARG-UNDEF', 'ARG0', 'ARG1', 'ARG2', 'ARG2-ATR', 'ARG2-GOL', 'ARG2-LOC', 'ARG2-SOU', 'ARG3', 'ARGM-ADV', 'ARGM-CAU', 'ARGM-DIR', 'ARGM-DIS', 'ARGM-EXT', 'ARGM-LOC', 'ARGM-MNR', 'ARGM-MNS', 'ARGM-MOD', 'ARGM-NEG', 'ARGM-PRP', 'ARGM-PRX', 'ARGM-TMP', nan]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "def encode_labels(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    le.fit(data)\n",
        "\n",
        "    encoded_data = le.transform(data)\n",
        "\n",
        "    uniq_labels = list(le.classes_)\n",
        "\n",
        "    return encoded_data, uniq_labels, le\n",
        "\n",
        "encoded_labels, uniq_labels, decoder = encode_labels(labels)\n",
        "data['srl'] = encoded_labels\n",
        "print(uniq_labels)\n",
        "# print(len(encoded_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icnQRH_EMPOb"
      },
      "source": [
        "**LABEL ENCODER**\n",
        "- For the labels we need to encode them, so using the LabelEncoder() we convert them into numerical values and store the encoded data.\n",
        "- The Fit(data) trains it on the data and then retrieves the unique labels after encoding.\n",
        "- Now instead of storing the labels directly we store them as the encoded form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiayKL9Zs45P",
        "outputId": "56fb7fe3-d11c-48a3-801c-3b752359e7b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14412, 300)\n"
          ]
        }
      ],
      "source": [
        "#redundant_cols = [ 'chunk', 'postposition', 'head-postag', 'dependency', 'is_arg', 'srl', 'predicate']\n",
        "#dropping all the unnecessary columns from the file\n",
        "redundant_cols = [ 'postposition', 'head-postag', 'is_arg', 'srl', 'predicate']\n",
        "x = data.drop(redundant_cols,axis = 1)\n",
        "#-->, 'predicate', 'dependency' --> redundant_cols = [ 'chunk', 'postposition', 'head-postag','is_arg', 'srl']\n",
        "x['chunk'] = LabelEncoder().fit_transform(x['chunk'])\n",
        "\n",
        "x['dependency'] = LabelEncoder().fit_transform(x['dependency'])\n",
        "\n",
        "\n",
        "\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CqRyMKIrMIs",
        "outputId": "488eb023-71db-4c4f-8876-653d9481b49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11529, 300) (2883, 300)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape,X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYs8en46rWYq",
        "outputId": "197b7e42-3c4d-4d78-a9da-4dea6bf61eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22 22 22 ... 22  2 22]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(max_iter = 2000)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbiw4VeDRUv4"
      },
      "source": [
        "## SRL USING THE LOGISTIC REGRESSION\n",
        "- Here we have used the logistic regression for the classification\n",
        "- We give the train data and the train labels for the fitting and then test the model upon the testing data and get the predictions.\n",
        "- And the predictions we got we compare it with the actaul results and give out the accuracy\n",
        "\n",
        "- In this case we get an accuracy of 62%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I_j6_UhvXSI"
      },
      "outputs": [],
      "source": [
        "y_pred_labels = decoder.inverse_transform(y_pred)\n",
        "y_test_labels = decoder.inverse_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1i6cqJnQ57t",
        "outputId": "90fc5fbb-a879-4129-d0b6-fd5b1f180aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ARGM-PRP' 'ARGM-PRP' nan nan nan nan nan 'ARG1' nan nan]\n",
            "[nan nan nan nan nan nan nan 'ARG1' nan nan]\n"
          ]
        }
      ],
      "source": [
        "print(y_test_labels[:10])\n",
        "print(y_pred_labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21gaKgXeQ57u",
        "outputId": "c35fc932-c1f6-477f-f909-539c13c655c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.45      0.16      0.23       189\n",
            "          10       0.00      0.00      0.00        20\n",
            "          11       0.00      0.00      0.00         6\n",
            "          12       0.00      0.00      0.00        16\n",
            "          13       0.00      0.00      0.00        18\n",
            "          14       0.33      0.04      0.07       133\n",
            "          15       0.40      0.03      0.06        62\n",
            "          16       0.00      0.00      0.00        10\n",
            "          19       0.00      0.00      0.00        17\n",
            "           2       0.61      0.30      0.40       397\n",
            "          21       0.38      0.16      0.22        69\n",
            "          22       0.65      0.94      0.77      1770\n",
            "           3       0.00      0.00      0.00        41\n",
            "           4       0.17      0.01      0.03        72\n",
            "           5       0.00      0.00      0.00        14\n",
            "           6       0.00      0.00      0.00         7\n",
            "           7       0.00      0.00      0.00         9\n",
            "           9       0.00      0.00      0.00        33\n",
            "\n",
            "    accuracy                           0.63      2883\n",
            "   macro avg       0.17      0.09      0.10      2883\n",
            "weighted avg       0.55      0.63      0.55      2883\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred_float = y_pred.astype(str)\n",
        "y_test_float = y_test.astype(str)\n",
        "\n",
        "# Now use classification_report\n",
        "print(classification_report(y_test_float, y_pred_float))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7wnCjRTQ57v",
        "outputId": "d924bd6d-84ea-4ff3-8aee-fa970d3debba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy when used logistic regression model 0.6344086021505376\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test_float, y_pred_float)\n",
        "\n",
        "print(\"Accuracy when used logistic regression model\",accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqGDOBUxd37D",
        "outputId": "47855c3f-0f20-4510-9474-d7ead57bc1a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = pd.DataFrame(classification_report(y_test_float, y_pred_float,output_dict=True)).transpose()\n",
        "class_report.to_csv(\"classification_report_Logistic_Regression.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF0HQbsFa3VP"
      },
      "source": [
        "# SRL using SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tNfELCRa2OP"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = clf.predict(X_test)\n",
        "\n",
        "y_pred_svm_labels = decoder.inverse_transform(y_pred_svm)\n",
        "y_test_labels = decoder.inverse_transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKcmpxQGcPZa",
        "outputId": "06a74dc4-d706-4d79-c9ce-ebe36a978385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.40      0.12      0.19       189\n",
            "           2       0.69      0.22      0.34       397\n",
            "           3       0.00      0.00      0.00        41\n",
            "           4       0.33      0.01      0.03        72\n",
            "           5       0.00      0.00      0.00        14\n",
            "           6       0.00      0.00      0.00         7\n",
            "           7       0.00      0.00      0.00         9\n",
            "           9       0.00      0.00      0.00        33\n",
            "          10       0.00      0.00      0.00        20\n",
            "          11       0.00      0.00      0.00         6\n",
            "          12       0.00      0.00      0.00        16\n",
            "          13       0.00      0.00      0.00        18\n",
            "          14       0.00      0.00      0.00       133\n",
            "          15       0.00      0.00      0.00        62\n",
            "          16       0.00      0.00      0.00        10\n",
            "          19       0.00      0.00      0.00        17\n",
            "          21       0.60      0.04      0.08        69\n",
            "          22       0.64      0.97      0.77      1770\n",
            "\n",
            "    accuracy                           0.63      2883\n",
            "   macro avg       0.15      0.08      0.08      2883\n",
            "weighted avg       0.54      0.63      0.53      2883\n",
            "\n",
            "Accuracy when used svm model 0.6330211585154353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "print(\"Accuracy when used svm model\",accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX6tU00gdf3g",
        "outputId": "eba3ab9a-f7fb-4b9c-a03f-7998b3906f76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = pd.DataFrame(classification_report(y_test, y_pred_svm,output_dict=True)).transpose()\n",
        "class_report.to_csv(\"classification_report_SVM.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le-x-RtzF0bT"
      },
      "source": [
        "- Instead of using the logistic regression or any other classifier we can make use of neural networks for this task.\n",
        "\n",
        "### SRL using Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6CV4-72R-DK",
        "outputId": "cd8b2290-50de-427a-e0ba-142d7cbd10e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
            "0  0.020622 -0.043543  0.017362  0.024762 -0.020766 -0.017783 -0.023139   \n",
            "1 -0.014496 -0.008514  0.001838 -0.013815 -0.023862  0.021470 -0.007265   \n",
            "2  0.021526 -0.021832  0.063153 -0.033886 -0.009450 -0.035176  0.080432   \n",
            "3 -0.047248  0.018531 -0.021601  0.054370 -0.024639  0.022130  0.068030   \n",
            "4  0.038407 -0.026316 -0.016674 -0.013441  0.014775  0.005128  0.034026   \n",
            "\n",
            "      dim_8     dim_9    dim_10  ...   dim_298   dim_299   dim_300  chunk  \\\n",
            "0  0.015411 -0.021622 -0.045250  ... -0.002109 -0.018631 -0.002061     NP   \n",
            "1  0.014412  0.024713  0.020350  ... -0.026965  0.007601  0.007057    NP2   \n",
            "2  0.158831 -0.014669 -0.037460  ...  0.101427 -0.090182  0.008483   VGNF   \n",
            "3 -0.017688  0.005726 -0.042218  ...  0.032187  0.065817  0.067730    NP3   \n",
            "4  0.008274 -0.023595 -0.039830  ...  0.047305 -0.050628 -0.014780    NP4   \n",
            "\n",
            "   postposition  head-postag   dependency  is_arg  srl  predicate  \n",
            "0            का          NP2           r6     0.0   22        NaN  \n",
            "1           NaN         VGNF           k2     1.0    2       VGNF  \n",
            "2         हो+एं          NP4  nmod__k1inv     0.0   22        NaN  \n",
            "3           में         VGNF          k7p     1.0   14       VGNF  \n",
            "4            का          VGF           k1     1.0    1        VGF  \n",
            "\n",
            "[5 rows x 307 columns]\n",
            "Train shape : (11025, 306) (11025,)\n",
            "Test shape : (1225, 306) (1225,)\n",
            "Test shape : (2162, 306) (2162,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def encode_labels(data):\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    le.fit(data)\n",
        "\n",
        "    encoded_data = le.transform(data)\n",
        "\n",
        "    uniq_labels = list(le.classes_)\n",
        "\n",
        "    return encoded_data, uniq_labels, le\n",
        "\n",
        "file_path = \"final_data.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "data['srl'] , classes , label_encoder = encode_labels(data['srl'])\n",
        "print(data.head())\n",
        "\n",
        "y_data = data['srl']\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(['srl'], axis=1), y_data , test_size=0.15, random_state=42)\n",
        "\n",
        "X_train , X_val  , y_train, y_val = train_test_split(X_train , y_train , test_size=0.1, random_state=None)\n",
        "print(\"Train shape :\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape :\", X_val.shape, y_val.shape)\n",
        "print(\"Test shape :\", X_test.shape, y_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqj-No1pXY3b"
      },
      "outputs": [],
      "source": [
        "def concat(x_data , y_data):\n",
        "  return pd.concat([x_data , y_data] , axis = 1)\n",
        "\n",
        "\n",
        "data_train = concat(X_train , y_train)\n",
        "data_val = concat(X_val , y_val)\n",
        "data_test = concat(X_test , y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL5U1ETsXnX_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self , data):\n",
        "    redundant_cols = [ 'chunk', 'postposition', 'head-postag', 'dependency', 'is_arg', 'srl', 'predicate']\n",
        "\n",
        "    x = data.drop(redundant_cols,axis = 1)\n",
        "\n",
        "    self.emb = x\n",
        "\n",
        "    self.emb = torch.Tensor(np.array(self.emb))\n",
        "    self.label  = torch.Tensor(data['srl'].values)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.emb)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x_data = self.emb[index]\n",
        "    y_data = self.label[index]\n",
        "\n",
        "    return x_data , y_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI36JqWkQ570"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 300\n",
        "NUM_HIDDEN_NODES =200\n",
        "NUM_OUTPUT_NODES = 5\n",
        "NUM_CLASSES = 23\n",
        "epochs = 50\n",
        "batchsize = 128\n",
        "learning_rate =0.00045"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VcgrLiHa20v"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(data_train)\n",
        "val_dataset = CustomDataset(data_val)\n",
        "test_dataset = CustomDataset(data_test)\n",
        "dataloader=DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=False)\n",
        "valloader = DataLoader(dataset=val_dataset , batch_size=batchsize,shuffle=False)\n",
        "testloader=DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfJiTdqfQ571"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "class SRL_LSTM(nn.Module):\n",
        "    def __init__(self,embeddings_dim,hidden_dim,output_dim,num_class,pretrained_embeddings=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        “pretrained_embeddings (numpy.array): previously trained word embeddings”\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = pretrained_embeddings\n",
        "\n",
        "        self.lstm = nn.LSTM(embeddings_dim,hidden_dim,num_layers=3,batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim,num_class)\n",
        "\n",
        "\n",
        "    def forward(self,pretrained_embeddings):\n",
        "        batch_size = pretrained_embeddings.size(1)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(pretrained_embeddings)\n",
        "\n",
        "        outputs = self.fc(outputs)\n",
        "\n",
        "\n",
        "        outputs = outputs.view(batch_size, 23)\n",
        "\n",
        "        outputs = F.log_softmax(outputs, dim=1)\n",
        "\n",
        "        return(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R67D7j-ZQ572"
      },
      "outputs": [],
      "source": [
        "def train_part(model , dataloader , optimizer , criterion , epoch):\n",
        "    total_loss = 0.0\n",
        "    total_acc=0.0\n",
        "\n",
        "    for input_word_dim, y in dataloader:\n",
        "        batch_size = input_word_dim.shape[0]\n",
        "\n",
        "        preds = model(input_word_dim.view([1,batch_size,300]))\n",
        "        y = y.type(torch.long)\n",
        "\n",
        "        loss = criterion(preds, y)\n",
        "\n",
        "        preds = torch.argmax(preds,dim=1)\n",
        "        #print(preds)\n",
        "        acc = sum(preds == y) / float(batch_size)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc+=acc.item()\n",
        "\n",
        "    print(\"train loss on epoch {epoch}  is {loss} and training accuracy {accuracy}\".format(epoch=epoch,loss=(total_loss/len(dataloader)),accuracy=(total_acc/len(dataloader))))\n",
        "    return total_loss , total_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF9iBIKQQ573",
        "outputId": "d2774d40-1967-416e-bc83-93379c6062e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n    test_loss = 0.0\\n    test_acc=0.0\\n    all_preds =np.zeros(0)\\n    all_y =  np.zeros(0)\\n    for input_word_dim,y in testloader:\\n        batch_size = input_word_dim.shape[0]\\n\\n        preds = model(input_word_dim.view([1,batch_size,300]))\\n        y = y.type(torch.long)\\n\\n        loss = criterion(preds, y)\\n        #print(\"Loss {}\".format(loss))\\n        #print(y)\\n        preds = torch.argmax(preds,dim=1)\\n        #print(preds)\\n        acc = sum(preds == y) / float(batch_size)\\n        #acc=model_accuracy(preds, y)\\n\\n\\n        all_preds = np.append(all_preds,np.array(preds))\\n        all_y = np.append(all_y,np.array(y))\\n\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        test_loss+=loss.item()\\n        test_acc+=acc.item() \\n\\n    print(\"test loss on epoch {epoch}  is {loss} and test accuracy {accuracy}\".format(epoch=epoch,loss=(test_loss/len(testloader)),accuracy=(test_acc/len(testloader))))\\n    return all_preds , all_y , test_loss , test_acc\\n'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test(model , testloader , optimizer , criterion , epoch):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    all_preds = []\n",
        "    all_y = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for input_word_dim, y in testloader:\n",
        "            batch_size = input_word_dim.shape[0]\n",
        "\n",
        "            preds = model(input_word_dim.view([1,batch_size,300]))\n",
        "            y = y.type(torch.long)\n",
        "\n",
        "            loss = criterion(preds, y)\n",
        "\n",
        "            # Accumulate loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "            correct = (preds == y).sum().item()\n",
        "            test_acc += correct / float(batch_size)\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_preds.extend(preds.tolist())\n",
        "            all_y.extend(y.tolist())\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = test_loss / len(testloader)\n",
        "    avg_acc = test_acc / len(testloader)\n",
        "\n",
        "    print(\"test loss on epoch {epoch} is {loss} and test accuracy {accuracy}\".format(epoch=epoch, loss=avg_loss, accuracy=avg_acc))\n",
        "\n",
        "    return all_preds, all_y, avg_loss, avg_acc\n",
        "'''\n",
        "    test_loss = 0.0\n",
        "    test_acc=0.0\n",
        "    all_preds =np.zeros(0)\n",
        "    all_y =  np.zeros(0)\n",
        "    for input_word_dim,y in testloader:\n",
        "        batch_size = input_word_dim.shape[0]\n",
        "\n",
        "        preds = model(input_word_dim.view([1,batch_size,300]))\n",
        "        y = y.type(torch.long)\n",
        "\n",
        "        loss = criterion(preds, y)\n",
        "        #print(\"Loss {}\".format(loss))\n",
        "        #print(y)\n",
        "        preds = torch.argmax(preds,dim=1)\n",
        "        #print(preds)\n",
        "        acc = sum(preds == y) / float(batch_size)\n",
        "        #acc=model_accuracy(preds, y)\n",
        "\n",
        "\n",
        "        all_preds = np.append(all_preds,np.array(preds))\n",
        "        all_y = np.append(all_y,np.array(y))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        test_loss+=loss.item()\n",
        "        test_acc+=acc.item()\n",
        "\n",
        "    print(\"test loss on epoch {epoch}  is {loss} and test accuracy {accuracy}\".format(epoch=epoch,loss=(test_loss/len(testloader)),accuracy=(test_acc/len(testloader))))\n",
        "    return all_preds , all_y , test_loss , test_acc\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLokBs8cQ574"
      },
      "outputs": [],
      "source": [
        "def train_model(model,dataloader,testloader,epochs,optimizer,criterion):\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss , total_acc = train_part(model , dataloader , optimizer , criterion , epoch)\n",
        "        train_acc_list.append((total_acc/len(dataloader)))\n",
        "        train_loss_list.append((total_loss/len(dataloader)))\n",
        "\n",
        "        all_preds , all_y , test_loss , test_acc = test(model , testloader , optimizer , criterion , epoch)\n",
        "        test_acc_list.append((test_acc/len(testloader)))\n",
        "        test_loss_list.append((test_loss/len(testloader)))\n",
        "\n",
        "        epoch_list.append(epoch)\n",
        "\n",
        "    return(train_loss_list,test_loss_list,train_acc_list,test_acc_list,all_preds,all_y,epoch_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R139iRtQ575"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = SRL_LSTM(embeddings_dim=EMBEDDING_DIM,hidden_dim=NUM_HIDDEN_NODES,output_dim =NUM_OUTPUT_NODES,num_class=NUM_CLASSES,pretrained_embeddings=None)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JsNaTgZQ576",
        "outputId": "baaf4484-ccb6-40c6-aa17-c1b3662b41c3"
      },
      "outputs": [],
      "source": [
        "train_loss, val_loss, train_acc, val_acc,preds,Y,epoch_list = train_model(model,dataloader,valloader,epochs,optimizer,criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SpSAePYQ577",
        "outputId": "e278945b-e014-4ccf-cace-dcc1b6aaf107"
      },
      "outputs": [],
      "source": [
        "test(model , testloader , optimizer , criterion , 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4idQLclFQ578",
        "outputId": "93da01f5-2ad4-4e00-b47d-a0e4e3b6f4dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.plot(epoch_list, train_acc)\n",
        "plt.plot(epoch_list,val_acc)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(['Train Accuracy','Val Accuracy'])\n",
        "#plt.savefig('/NN_train_val_accuracy_bilstm_50e.png')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(epoch_list, train_loss)\n",
        "plt.plot(epoch_list,val_loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(['Train Loss','Val Loss'])\n",
        "#plt.savefig('./NN_train_val_loss_bilstm_50e.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deLG0vwnQ578",
        "outputId": "639ae8c8-7afd-4040-a590-b798f11fce5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\satya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\satya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\satya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = pd.DataFrame(classification_report(Y, preds,output_dict=True)).transpose()\n",
        "class_report.to_csv(\"./outputs/classification_report_LSTM.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
