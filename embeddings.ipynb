{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvfOdM9ULxZk",
        "outputId": "990e7aa3-1529-4766-b57f-c783c5bae3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file has been created successfully.\n",
            "14412 14412\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = \"/content/preprocessed_dataset.txt\"\n",
        "output_file = \"data.csv\"\n",
        "\n",
        "# Column names for the CSV file\n",
        "header = [\"word\", \"chunk\", \"postposition\", \"head-postag\", \"dependency\", \"is_arg\", \"srl\", \"predicate\"]\n",
        "\n",
        "# Function to parse each line and extract relevant information\n",
        "def parse_line(line):\n",
        "    elements = line.split()\n",
        "    word = elements[0] if len(elements) > 0 else \"\"\n",
        "    chunk = elements[1] if len(elements) > 1 else \"\"\n",
        "    postposition = elements[2].split('_')[1] if len(elements) > 2 and '_' in elements[2] else \"\"\n",
        "    head_postag = elements[3] if len(elements) > 3 else \"\"\n",
        "    dependency = elements[4] if len(elements) > 4 else \"\"\n",
        "    is_arg = elements[5] if len(elements) > 5 else \"\"\n",
        "    srl = elements[6] if len(elements) > 6 else \"\"\n",
        "    predicate = elements[7] if len(elements) > 7 else \"\"\n",
        "    return [word, chunk, postposition, head_postag, dependency, is_arg, srl, predicate]\n",
        "\n",
        "\n",
        "\n",
        "cnt = 0\n",
        "validlines =0\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
        "    reader = infile.readlines()\n",
        "    writer = csv.writer(outfile)\n",
        "    writer.writerow(header)\n",
        "    for line in reader:\n",
        "        cnt+=1\n",
        "        row = parse_line(line.strip())\n",
        "        if row is not None:  # Check if row is not None before writing\n",
        "            validlines+=1\n",
        "            writer.writerow(row)\n",
        "\n",
        "print(\"CSV file has been created successfully.\")\n",
        "print(validlines,cnt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: readcsv file using pandas and display head\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ7p4NHRPZ7y",
        "outputId": "de3e18eb-bcbe-4100-d178-bf728be39f74"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            word chunk postposition head-postag   dependency  is_arg  \\\n",
            "0          सरकार    NP           का         NP2           r6     0.0   \n",
            "1         भूमिका   NP2          NaN        VGNF           k2     1.0   \n",
            "2         अपनाते  VGNF        हो+एं         NP4  nmod__k1inv     0.0   \n",
            "3         झारखंड   NP3          में        VGNF          k7p     1.0   \n",
            "4      छापामारों   NP4           का         VGF           k1     1.0   \n",
            "...          ...   ...          ...         ...          ...     ...   \n",
            "14407      माहौल   NP5          NaN        VGNN          pof     0.0   \n",
            "14408      बनाने  VGNN           के         VGF           rt     1.0   \n",
            "14409        देश   NP6          में         VGF          k7p     1.0   \n",
            "14410     अभियान   NP7          NaN         VGF           k2     1.0   \n",
            "14411    चलाएंगे   VGF          NaN           0         root     0.0   \n",
            "\n",
            "            srl predicate  \n",
            "0           NaN       NaN  \n",
            "1          ARG1      VGNF  \n",
            "2           NaN       NaN  \n",
            "3      ARGM-LOC      VGNF  \n",
            "4          ARG0       VGF  \n",
            "...         ...       ...  \n",
            "14407       NaN       NaN  \n",
            "14408  ARGM-PRP       VGF  \n",
            "14409  ARGM-LOC       VGF  \n",
            "14410      ARG1       VGF  \n",
            "14411       NaN       NaN  \n",
            "\n",
            "[14412 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aA2Ali7Wn0H",
        "outputId": "c9ed2658-ea9b-4366-e60a-a60918b0bc53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227139 sha256=316aafee42bc7fbb4cf3e06c52984beb53904237a1268682fcdd3349d437b380\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install progressbar2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGSwe4RhZGXY",
        "outputId": "dc524707-90e0-4f78-ab77-02ba5f613dac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.0.0->progressbar2) (4.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "\n",
        "# Download the Hindi fastText model if it hasn't been downloaded already\n",
        "fasttext.util.download_model('hi', if_exists='ignore')  # Hindi\n",
        "\n",
        "\n",
        "ft_hindi = fasttext.load_model('cc.hi.300.bin')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIdl-RshWdYF",
        "outputId": "85d2d603-d79f-4d08-ba9a-2be06d0e85a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def generate_embedding(word):\n",
        "    try:\n",
        "        embedding = ft_hindi.get_word_vector(word)\n",
        "        return embedding\n",
        "    except KeyError:\n",
        "        return [0.0] * 300\n",
        "\n",
        "data = []\n",
        "with open(\"data.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        word = row[\"word\"]\n",
        "        embedding = generate_embedding(word)\n",
        "        data.append({\"word\": embedding})\n",
        "\n",
        "\n",
        "output_file = \"embeddings.csv\"\n",
        "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    fieldnames = [\"word\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for row in data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(\"Embeddings have been written to embeddings.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeuznSgZVGvE",
        "outputId": "8ac5537a-bc31-4dde-97ee-3e1dbf598da9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings have been written to embeddings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('embeddings.csv')"
      ],
      "metadata": {
        "id": "MrXpvcdmy9li"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"word\"] = df[\"word\"].str.strip(\"[]\")\n",
        "\n",
        "# Split each embedding string into individual dimensions\n",
        "df[\"word\"] = df[\"word\"].apply(lambda x: x.split())\n",
        "\n",
        "# Expand the list of dimensions into separate columns\n",
        "embedding_df = df[\"word\"].apply(pd.Series)\n",
        "\n",
        "embedding_df.columns = [f\"dim_{i+1}\" for i in range(embedding_df.shape[1])]\n",
        "\n",
        "# Concatenate the original DataFrame with the new DataFrame containing embeddings\n",
        "result_df = pd.concat([df.drop(columns=[\"word\"]), embedding_df], axis=1)\n",
        "\n",
        "result_df.to_csv(\"final_data.csv\", index=False)\n",
        "\n",
        "print(\"Embeddings with dimensions have been written to embeddings_with_dimensions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-YQoAWww9e6",
        "outputId": "5e5a752a-efcf-476f-ce67-a5f1691ba6fb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings with dimensions have been written to embeddings_with_dimensions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = pd.read_csv(\"embeddings_with_dimensions.csv\")\n",
        "\n",
        "data_df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "merged_df = pd.concat([embeddings_df, data_df.iloc[:, 1:]], axis=1)  # Assuming additional columns start from the second column\n",
        "\n",
        "merged_df.to_csv(\"merged_embeddings_data.csv\", index=False)\n",
        "\n",
        "print(\"Merged data with embeddings have been written to merged_embeddings_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwPQ-pl9xhSf",
        "outputId": "7778e2f7-f191-498c-d8e1-df43ea94c453"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data with embeddings have been written to merged_embeddings_data.csv\n"
          ]
        }
      ]
    }
  ]
}